{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search = GridSearchCV(pipeline, param_grid_log, cv=5, scoring='roc_auc', return_train_score=True)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# results = grid_search.cv_results_\n",
    "\n",
    "# scoring = {\"AUC\": \"roc_auc\", \"my_f10_score\": f10_score} # préciser quel score utiliser\n",
    "\n",
    "# mlflow.set_experiment(\"Classification Pipeline\")\n",
    "# with mlflow.start_run():\n",
    "  \n",
    "\n",
    "# Accéder aux informations de chaque combinaison de paramètres testée\n",
    "# for i in range(len(results['params'])):\n",
    "#   print(\"Paramètres:\", results['params'][i])\n",
    "#   print(\"Moyenne du score :\", results['mean_test_score'][i])\n",
    "#   print(\"Écart-type du score :\", results['std_test_score'][i])\n",
    "#   print()\n",
    "\n",
    "      # Entraînement du modèle avec les paramètres spécifiés\n",
    "      # pipeline.set_params(**params)\n",
    "\n",
    "        # score = cross validate (pipeline,xtrain, ytrain, cv) auc f10score\n",
    "\n",
    "        # score validation\n",
    "        # score test\n",
    "\n",
    "        # pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Évaluation du modèle sur les données de test\n",
    "        # y_pred = pipeline.predict(X_test)\n",
    "        # accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # Enregistrement des métriques dans MLflow pour chaque modèle/scaler testé\n",
    "        # mlflow.log_param(\"model\", str(pipeline.steps[-1][1]))  # Nom du modèle\n",
    "        # mlflow.log_param(\"params\", str(params))  # Paramètres du modèle\n",
    "        # mlflow.log_metric(\"accuracy\", accuracy)  # Métrique d'évaluation\n",
    "\n",
    "  # grid_search.fit(X_train, y_train)\n",
    "  # best_model = grid_search.best_estimator_\n",
    "  # f10_score = best_model.score(X_test, y_test)\n",
    "\n",
    "  # \n",
    "\n",
    "# mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements.txt\n",
    "\n",
    "# pyqt5<5.16\n",
    "# pyqtwebengine<5.16\n",
    "# daal==2021.2.3\n",
    "# pytz<2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
