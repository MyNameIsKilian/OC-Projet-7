{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "from flask import Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def main():\n",
    "    \n",
    "    response = {\n",
    "        'dataset': 'iris',\n",
    "        'model_parameters': 'json_params',\n",
    "        'accuracy': 'accuracy'\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': str(response),\n",
    "        'headers': {\n",
    "            \"Content-Type\": \"application/json; charset=utf-8\"\n",
    "        }\n",
    "    }\n",
    "    # return \"<p>Hello, World!</p>\"\n",
    "\n",
    "# def loadData():\n",
    "\n",
    "#     print(\"function load data start\")\n",
    "#     # Charger le dataset Iris\n",
    "#     iris = load_iris()\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "#     model = LogisticRegression()\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     model_params = model.get_params()\n",
    "#     json_params = json.dumps(model_params)\n",
    "    \n",
    "#     # load model from s3\n",
    "\n",
    "#     print(\"function load data end\")\n",
    "\n",
    "#     response = {\n",
    "#         'dataset': 'iris',\n",
    "#         'model_parameters': json_params,\n",
    "#         'accuracy': accuracy\n",
    "#     }\n",
    "\n",
    "#     return response\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search = GridSearchCV(pipeline, param_grid_log, cv=5, scoring='roc_auc', return_train_score=True)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# results = grid_search.cv_results_\n",
    "\n",
    "# scoring = {\"AUC\": \"roc_auc\", \"my_f10_score\": f10_score} # préciser quel score utiliser\n",
    "\n",
    "# mlflow.set_experiment(\"Classification Pipeline\")\n",
    "# with mlflow.start_run():\n",
    "  \n",
    "\n",
    "# Accéder aux informations de chaque combinaison de paramètres testée\n",
    "# for i in range(len(results['params'])):\n",
    "#   print(\"Paramètres:\", results['params'][i])\n",
    "#   print(\"Moyenne du score :\", results['mean_test_score'][i])\n",
    "#   print(\"Écart-type du score :\", results['std_test_score'][i])\n",
    "#   print()\n",
    "\n",
    "      # Entraînement du modèle avec les paramètres spécifiés\n",
    "      # pipeline.set_params(**params)\n",
    "\n",
    "        # score = cross validate (pipeline,xtrain, ytrain, cv) auc f10score\n",
    "\n",
    "        # score validation\n",
    "        # score test\n",
    "\n",
    "        # pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Évaluation du modèle sur les données de test\n",
    "        # y_pred = pipeline.predict(X_test)\n",
    "        # accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # Enregistrement des métriques dans MLflow pour chaque modèle/scaler testé\n",
    "        # mlflow.log_param(\"model\", str(pipeline.steps[-1][1]))  # Nom du modèle\n",
    "        # mlflow.log_param(\"params\", str(params))  # Paramètres du modèle\n",
    "        # mlflow.log_metric(\"accuracy\", accuracy)  # Métrique d'évaluation\n",
    "\n",
    "  # grid_search.fit(X_train, y_train)\n",
    "  # best_model = grid_search.best_estimator_\n",
    "  # f10_score = best_model.score(X_test, y_test)\n",
    "\n",
    "  # \n",
    "\n",
    "# mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
