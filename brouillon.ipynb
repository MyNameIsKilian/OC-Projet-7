{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic reg cross validate\n",
    "{'fit_time': array([26.64608026, 26.87596512, 12.91484284, 20.69194603, 25.04282022]),\n",
    " 'score_time': array([0.75971341, 0.20326853, 0.21254563, 0.20427799, 0.21911764]),\n",
    " 'test_score': array([0.63045007, 0.64654634, 0.6513145 , 0.65986953, 0.60752373]),\n",
    " 'train_score': array([0.75662333, 0.75818348, 0.73804827, 0.75032474, 0.75789132])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm cross validate avec std scaler\n",
    "{'fit_time': array([5.12360835, 2.37041664, 2.41124582, 2.05963945, 1.90335631]),\n",
    " 'score_time': array([0.20288706, 0.60501051, 0.2047143 , 0.19802332, 0.19889545]),\n",
    " 'test_score': array([0.76164244, 0.68518519, 0.70971522, 0.72366108, 0.66385406]),\n",
    " 'train_score': array([1., 1., 1., 1., 1.])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfc cross validate avec std scaler\n",
    "{'fit_time': array([1.70818114, 1.21364379, 1.19071674, 1.20064211, 1.18891668]),\n",
    " 'score_time': array([0.21861148, 0.21948099, 0.20915508, 0.20816493, 0.20764995]),\n",
    " 'test_score': array([0.74281138, 0.69882013, 0.67410836, 0.73157155, 0.69891402]),\n",
    " 'train_score': array([1., 1., 1., 1., 1.])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns scaled with MinMaxScaler()\n",
    "# Train shape: (24600, 490) Valid shape: (6151, 490)\n",
    "# GridSearch created, training started with model : LogisticRegression()\n",
    "# Best parameters: {'C': 0.1, 'class_weight': 'balanced', 'max_iter': 5000, 'solver': 'saga'}\n",
    "# Best score: 0.6561164979335288\n",
    "# Columns scaled with MinMaxScaler()\n",
    "# Train shape: (24600, 490) Valid shape: (6151, 490)\n",
    "# GridSearch created, training started with model : LGBMClassifier()\n",
    "# Best parameters: {'class_weight': 'balanced', 'learning_rate': 0.02, 'n_estimators': 400}\n",
    "# Best score: 0.4962365331725184\n",
    "# Columns scaled with MinMaxScaler()\n",
    "# Train shape: (24600, 490) Valid shape: (6151, 490)\n",
    "# GridSearch created, training started with model : RandomForestClassifier()\n",
    "# Best parameters: {'class_weight': 'balanced', 'max_depth': 11, 'n_estimators': 100}\n",
    "# Best score: 0.21220362164741635\n",
    "\n",
    "# Columns scaled with RobustScaler()\n",
    "# Train shape: (24600, 490) Valid shape: (6151, 490)\n",
    "# GridSearch created, training started with model : LogisticRegression()\n",
    "# Best parameters: {'C': 0.1, 'class_weight': 'balanced', 'max_iter': 5000, 'solver': 'saga'} => max iter atteint\n",
    "# Best score: 0.6835492987221139\n",
    "# Columns scaled with RobustScaler()\n",
    "# Train shape: (24600, 490) Valid shape: (6151, 490)\n",
    "# GridSearch created, training started with model : LGBMClassifier()\n",
    "# Best parameters: {'class_weight': 'balanced', 'learning_rate': 0.02, 'n_estimators': 400}\n",
    "# Best score: 0.4977519793069881\n",
    "# Columns scaled with RobustScaler()\n",
    "# Train shape: (24600, 490) Valid shape: (6151, 490)\n",
    "# GridSearch created, training started with model : RandomForestClassifier()\n",
    "# Best parameters: {'class_weight': 'balanced', 'max_depth': 11, 'n_estimators': 100}\n",
    "# Best score: 0.2091916655882907\n",
    "\n",
    "# Columns scaled with StandardScaler()\n",
    "# Train shape: (24600, 490) Valid shape: (6151, 490)\n",
    "# GridSearch created, training started with model : LogisticRegression()\n",
    "# Best parameters: {'C': 0.1, 'class_weight': 'balanced', 'max_iter': 5000, 'solver': 'saga'} max iter atteint\n",
    "# Best score: 0.6417426552173605\n",
    "# Columns scaled with StandardScaler()\n",
    "# Train shape: (24600, 490) Valid shape: (6151, 490)\n",
    "# GridSearch created, training started with model : LGBMClassifier()\n",
    "# Best parameters: {'class_weight': 'balanced', 'learning_rate': 0.02, 'n_estimators': 400}\n",
    "# Best score: 0.49303568638747003\n",
    "# Columns scaled with StandardScaler()\n",
    "# Train shape: (24600, 490) Valid shape: (6151, 490)\n",
    "# GridSearch created, training started with model : RandomForestClassifier()\n",
    "# Best parameters: {'class_weight': 'balanced', 'max_depth': 11, 'n_estimators': 100}\n",
    "# Best score: 0.22011250381594444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search = GridSearchCV(pipeline, param_grid_log, cv=5, scoring='roc_auc', return_train_score=True)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# results = grid_search.cv_results_\n",
    "\n",
    "# scoring = {\"AUC\": \"roc_auc\", \"my_f10_score\": f10_score} # préciser quel score utiliser\n",
    "\n",
    "# mlflow.set_experiment(\"Classification Pipeline\")\n",
    "# with mlflow.start_run():\n",
    "  \n",
    "\n",
    "# Accéder aux informations de chaque combinaison de paramètres testée\n",
    "# for i in range(len(results['params'])):\n",
    "#   print(\"Paramètres:\", results['params'][i])\n",
    "#   print(\"Moyenne du score :\", results['mean_test_score'][i])\n",
    "#   print(\"Écart-type du score :\", results['std_test_score'][i])\n",
    "#   print()\n",
    "\n",
    "      # Entraînement du modèle avec les paramètres spécifiés\n",
    "      # pipeline.set_params(**params)\n",
    "\n",
    "        # score = cross validate (pipeline,xtrain, ytrain, cv) auc f10score\n",
    "\n",
    "        # score validation\n",
    "        # score test\n",
    "\n",
    "        # pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Évaluation du modèle sur les données de test\n",
    "        # y_pred = pipeline.predict(X_test)\n",
    "        # accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # Enregistrement des métriques dans MLflow pour chaque modèle/scaler testé\n",
    "        # mlflow.log_param(\"model\", str(pipeline.steps[-1][1]))  # Nom du modèle\n",
    "        # mlflow.log_param(\"params\", str(params))  # Paramètres du modèle\n",
    "        # mlflow.log_metric(\"accuracy\", accuracy)  # Métrique d'évaluation\n",
    "\n",
    "  # grid_search.fit(X_train, y_train)\n",
    "  # best_model = grid_search.best_estimator_\n",
    "  # f10_score = best_model.score(X_test, y_test)\n",
    "\n",
    "  # \n",
    "\n",
    "# mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"Metrics LGBM Classifier after Grid Search CV\")\n",
    "with mlflow.start_run():\n",
    "  \n",
    "  # Accéder aux informations de chaque combinaison de paramètres testée\n",
    "    for i in range(len(results['params'])):\n",
    "        # print(\"Paramètres:\", results['params'][i])\n",
    "        mlflow.log_param(\"Params\", results['params'][i])\n",
    "        mlflow.log_metric(\"mean_train_AUC\", results['mean_train_AUC'][i])\n",
    "        mlflow.log_metric(\"mean_test_AUC\", results['mean_test_AUC'][i])\n",
    "        mlflow.log_metric(\"mean_train_my_f10_score\", results['mean_train_my_f10_score'][i])\n",
    "        mlflow.log_metric(\"mean_test_my_f10_score\", results['mean_test_my_f10_score'][i])\n",
    "        print()\n",
    "\n",
    "    # Évaluation du modèle sur les données de test\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    mlflow.log_param(\"Best Model\", best_model)\n",
    "    mlflow.log_param(\"Best Model params\", best_model.get_params)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
